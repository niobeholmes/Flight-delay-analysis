{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources:http://nbviewer.jupyter.org/url/ddowey.github.io/cs109-Final-Project/FlightDelay-ProjectFinal.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a frequent traveller, I have spent a lot of hours at airports, waiting for delayed flights. \n",
    "This analysis has 2 main goals:<br> \n",
    "<br> \n",
    "1) Predict whether a flight will be delayed I will approach the problem as a binary classification problem and try to predict whether a flight will be delayed or not and which model is the best to predict flight delays. <br> \n",
    "<br> \n",
    "2) Examine the causes of flight delays. I will also focus on exploratory analysis and determine what is the relationship between the available variables (temporal, spatial and other). Flights belonging to which airline are most likely to be delayed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion: prediction likely requires more features than those available. For further information, I would add historical information on weather in each of the airports' locations- I would expect that flights are more delayed when it snows or during a particularly rainy season.\n",
    "Another interesting information would be information about the state of the plane <br>\n",
    "<br>\n",
    "Possible extension:coupon-specific information for each domestic itinerary of the Origin and Destination Survey, such as the operating carrier, origin and destination airports, number of passengers, fare class, coupon type, trip break indicator, and distance.\n",
    "https://www.transtats.bts.gov/Tables.asp?DB_ID=125&DB_Name=Airline%20Origin%20and%20Destination%20Survey%20%28DB1B%29&DB_Short_Name=Origin%20and%20Destination%20Survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to download the data from the Bureau of Transportation Statistics. There does not seem to be any API, but the complete dataset can be downloaded by months from the URL in the following format:  https://transtats.bts.gov/PREZIP/On_Time_On_Time_Performance_YYYY_M.zip <br>\n",
    "<br>\n",
    "Due to the size of the dataset, I'd rather download only specific fields. I will select them and download the data using Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load basic libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import zipfile\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The years and months to download can be changed easily. For now, I will download 2015 data. Later on, I can download and analyze more years or use 2016 data to test my model on data to imitate a real-life implementation. <br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = [\"2016\"] # years to download\n",
    "months = [str(month+1) for month in range(12)] # months to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = os.getcwd() # current folder\n",
    "target_folder = 'bts_data' # download the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\micakova\\\\Experiments\\\\Pokusy\\\\Search'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, I will check what fields I can download. The full dataset contains a lot of columns. To make the volume of downloaded data smaller, I will specify which variables I would like to download as opposed to downloading the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_url = 'https://transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time'\n",
    "page = requests.get(target_url) \n",
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "fields = list()\n",
    "#[x for x in soup.findAll(type='checkbox')]\n",
    "for x in soup.findAll(type='checkbox'):\n",
    "    try:\n",
    "        fields.append(x['title'])\n",
    "    except Exception:\n",
    "        pass     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'UniqueCarrier',\n",
       " 'AirlineID',\n",
       " 'Carrier',\n",
       " 'TailNum',\n",
       " 'FlightNum',\n",
       " 'OriginAirportID',\n",
       " 'OriginAirportSeqID',\n",
       " 'OriginCityMarketID',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'OriginStateFips',\n",
       " 'OriginStateName',\n",
       " 'OriginWac',\n",
       " 'DestAirportID',\n",
       " 'DestAirportSeqID',\n",
       " 'DestCityMarketID',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'DestStateFips',\n",
       " 'DestStateName',\n",
       " 'DestWac',\n",
       " 'CRSDepTime',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'DepDel15',\n",
       " 'DepartureDelayGroups',\n",
       " 'DepTimeBlk',\n",
       " 'TaxiOut',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'TaxiIn',\n",
       " 'CRSArrTime',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'ArrDel15',\n",
       " 'ArrivalDelayGroups',\n",
       " 'ArrTimeBlk',\n",
       " 'Cancelled',\n",
       " 'CancellationCode',\n",
       " 'Diverted',\n",
       " 'CRSElapsedTime',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'DistanceGroup',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'FirstDepTime',\n",
       " 'TotalAddGTime',\n",
       " 'LongestAddGTime',\n",
       " 'DivAirportLandings',\n",
       " 'DivReachedDest',\n",
       " 'DivActualElapsedTime',\n",
       " 'DivArrDelay',\n",
       " 'DivDistance',\n",
       " 'Div1Airport',\n",
       " 'Div1AirportID',\n",
       " 'Div1AirportSeqID',\n",
       " 'Div1WheelsOn',\n",
       " 'Div1TotalGTime',\n",
       " 'Div1LongestGTime',\n",
       " 'Div1WheelsOff',\n",
       " 'Div1TailNum',\n",
       " 'Div2Airport',\n",
       " 'Div2AirportID',\n",
       " 'Div2AirportSeqID',\n",
       " 'Div2WheelsOn',\n",
       " 'Div2TotalGTime',\n",
       " 'Div2LongestGTime',\n",
       " 'Div2WheelsOff',\n",
       " 'Div2TailNum',\n",
       " 'Div3Airport',\n",
       " 'Div3AirportID',\n",
       " 'Div3AirportSeqID',\n",
       " 'Div3WheelsOn',\n",
       " 'Div3TotalGTime',\n",
       " 'Div3LongestGTime',\n",
       " 'Div3WheelsOff',\n",
       " 'Div3TailNum',\n",
       " 'Div4Airport',\n",
       " 'Div4AirportID',\n",
       " 'Div4AirportSeqID',\n",
       " 'Div4WheelsOn',\n",
       " 'Div4TotalGTime',\n",
       " 'Div4LongestGTime',\n",
       " 'Div4WheelsOff',\n",
       " 'Div4TailNum',\n",
       " 'Div5Airport',\n",
       " 'Div5AirportID',\n",
       " 'Div5AirportSeqID',\n",
       " 'Div5WheelsOn',\n",
       " 'Div5TotalGTime',\n",
       " 'Div5LongestGTime',\n",
       " 'Div5WheelsOff',\n",
       " 'Div5TailNum']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a first step, I will select the following variables:** <br>\n",
    "<br>\n",
    "1) **Time Period**: ['Year','Month','DayofMonth','DayOfWeek'] <br>\n",
    "2) **Airline**: ['UniqueCarrier',' FlightNum']  <br>\n",
    "3) **Origin and destination **: ['OriginAirportID','DestAirportID','OriginWac','DestWac']<br>\n",
    "4) **Departure and arrival performance +  flight summary information**: [CRSArrTime','CRSDepTime','DepDelay','ArrDelay','Cancelled','CancellationCode','CRSElapsedTime','Distance','DivAirportLandings'] <br>\n",
    "5) **Detailed information about the cause of delay**: ['CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\t\n",
    "\n",
    "<br> For the prediction itself, I will only use information which is available weeks/months ahead of the flight. However, as a part of the exploratory analysis, I will also examine variables which are only available ex-post to examine the reasons for the delay and the relationship between departure and arrival delay. <br> \n",
    "I tried to minimize duplicities (downloading only one from OriginAirportID and OriginAirportSeqID is sufficient), but some may still remain. If I discover that some of the fields are redundant during the analysis, I will omit them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unselect_vars = ['OriginAirportSeqID','DestAirportSeqID'] # Some of the fields are already preselected \n",
    "\n",
    "selected_vars = ['Year','Month','DayofMonth','DayOfWeek'\n",
    ",'UniqueCarrier'\\\n",
    ",'OriginWac','DestWac' \\\n",
    ",'FlightNum'\\\n",
    ",'CRSArrTime','CRSDepTime'\\\n",
    ",'DepDelay','ArrDelay' \\\n",
    ",'Cancelled','CancellationCode' \\\n",
    ",'CRSElapsedTime','Distance'  \\\n",
    ",'DivAirportLandings'\n",
    ",'CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay'] # list of fields to select\n",
    "selected_vars = selected_vars + unselect_vars # OriginAirportID selected by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'OriginWac', 'DestWac', 'FlightNum', 'CRSArrTime', 'CRSDepTime', 'DepDelay', 'ArrDelay', 'Cancelled', 'CancellationCode', 'CRSElapsedTime', 'Distance', 'DivAirportLandings', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'OriginAirportSeqID', 'DestAirportSeqID']\n"
     ]
    }
   ],
   "source": [
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " Possible other data to use:other data: latitude, longitude, weather data, airplane data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_data(root_path, target_folder, years, months, selected_vars=None):\n",
    "    \"\"\"\n",
    "    Download the BTS data using selenium. \n",
    "    years, months, selected_vars are lists. \n",
    "    selected_vars is optional - if not specified, all available fields will be downloaded.\n",
    "    \"\"\"\n",
    "    target_folder_path = os.path.join(root_path,target_folder)\n",
    "    if os.path.exists(target_folder_path):\n",
    "        print('folder already exists!')\n",
    "    else:\n",
    "        #specify target_folder, years and months (list) and selected_vars (optional)\n",
    "        profile = webdriver.FirefoxProfile() #create a new profile\n",
    "        profile.set_preference(\"browser.download.folderList\", 2)\n",
    "        profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        profile.set_preference(\"browser.download.dir\", target_folder_path) # save data to\n",
    "        profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/x-gzip\")\n",
    "        target_url = 'https://transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time'\n",
    "        driver = webdriver.Firefox(firefox_profile=profile)\n",
    "        driver.get(target_url)\n",
    "        time.sleep(10)\n",
    "        if selected_vars:\n",
    "            for element in selected_vars:\n",
    "                driver.find_element_by_xpath(\"//input[@type='checkbox' and @title='{}']\".format(element)).click()\n",
    "        else:  \n",
    "            driver.find_element_by_name(\"DownloadZip\").click() # download all fields\n",
    "        time.sleep(10)\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                print(\"Downloading data for {}/{}\".format(month, year))\n",
    "                Select(driver.find_element_by_id('XYEAR')).select_by_value(year) # select year in the dropdown list\n",
    "                time.sleep(5)\n",
    "                Select(driver.find_element_by_id('FREQUENCY')).select_by_value(month) # select month in the dropdown list\n",
    "                time.sleep(5)\n",
    "                driver.find_element_by_name(\"Download\").click() \n",
    "                time.sleep(20) # wait \n",
    "                #time.sleep(10)\n",
    "        time.sleep(150)  # wait to  make sure all the files have finished downloading  \n",
    "        print('Driver is quitting...')\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unzips individual files, renames them and moves them to a single folder \n",
    "\n",
    "def unzip_data(main_folder_path,target_folder_name):\n",
    "    file_extension = 'zip'\n",
    "    new_folder_name = 'unzipped'\n",
    "    target_folder = os.path.join(main_folder_path,target_folder_name)\n",
    "    print('target folder: {}'.format(target_folder))\n",
    "    new_folder = os.path.join(target_folder,new_folder_name)\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "        for i,item in enumerate(os.listdir(target_folder)): # loop through items in dir.\n",
    "            if item.endswith(file_extension): # check for \".zip\" extension\n",
    "                zip_ref = zipfile.ZipFile(os.path.join(target_folder,item)) # create zipfile object\n",
    "                zip_ref.extractall(os.path.join(new_folder,new_folder_name+'{}'.format(i+1)))\n",
    "                zip_ref.close()\n",
    "                print('{} unzipped'.format(item))\n",
    "            # extract file to dir. The unzipped files have the same name\n",
    "        #for subfolder in os.listdir(new_folder): # rename file if the files have the same name\n",
    "        for subfolder in next(os.walk(new_folder))[1]:\n",
    "            subfolder_path = os.path.join(new_folder,subfolder)\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                #print(file_name)\n",
    "                if re.search('unzipped[0-9]+', subfolder_path).group(0) is None:\n",
    "                    new_file_name = os.path.join(new_folder,'data0.csv')\n",
    "                else: \n",
    "                    new_file_name = os.path.join(new_folder,'data{}.csv'.format(re.search('unzipped[0-9]+', subfolder_path).group(0)))   \n",
    "                #print(new_file_name)\n",
    "                os.rename(os.path.join(subfolder_path,file_name), os.path.join(new_file_name))\n",
    "            \n",
    "         \n",
    "            # extract file to single dir\n",
    "    else:\n",
    "        print('The directory {} already exists!'.format(new_folder))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_folder = os.path.join(main_folder_path,target_folder_name)\n",
    "# mf path C:\\\\Users\\\\micakova\\\\Experiments\\\\Pokusy\\\\Search\n",
    "# target folder name data_train_raw\\\\\n",
    "# unzipped1...12\n",
    "def concat_files(main_folder_path,target_folder_name,output_name):\n",
    "    write_header = True\n",
    "    new_folder_path = os.path.join(main_folder_path,target_folder_name,'unzipped')\n",
    "    print(new_folder_path)\n",
    "    if os.path.exists(output_name):\n",
    "        print('file already exists!')\n",
    "    else:\n",
    "        with open(output_name, 'w') as target:\n",
    "            #for item in os.listdir(target_folder+ '\\\\' + new_folder_name):\n",
    "            csv_files = [file for file in os.listdir(new_folder_path) if file.endswith('csv')]\n",
    "            for item in csv_files:\n",
    "                print(item)\n",
    "                with open(os.path.join(new_folder_path,item), 'r') as source:\n",
    "                    lines = source.readlines()\n",
    "                    if not write_header:\n",
    "                        lines = lines[1:]\n",
    "                    else:\n",
    "                        write_header = False\n",
    "                    lines = [line for line in lines if line.strip() != '']\n",
    "                    target.writelines(lines)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_data(root, target_folder, years, months, selected_vars=selected_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target folder: C:\\Users\\micakova\\Experiments\\Pokusy\\Search\\bts_data\n"
     ]
    }
   ],
   "source": [
    "unzip_data(root,target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_name = 'df_2016.csv'\n",
    "concat_files(root, target_folder,output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2015 = pd.read_csv(output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
